{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Final.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"rGeLP-oiS21j","outputId":"d531471d-7a77-4fb1-dbfa-d170320d9720","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1574699660532,"user_tz":-60,"elapsed":1995,"user":{"displayName":"Susi Handayani","photoUrl":"","userId":"17500954646245155216"}}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K5_baA0dTZVC","outputId":"1104e846-49e7-4e42-b38d-0dfac037288d","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1574699663420,"user_tz":-60,"elapsed":4853,"user":{"displayName":"Susi Handayani","photoUrl":"","userId":"17500954646245155216"}}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, ConvLSTM2D, BatchNormalization, Dropout, MaxPooling2D, Bidirectional\n","from tensorflow.keras import Model\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pylab as plt\n","import matplotlib.pylab as plt \n","import os\n","import time\n","from tabulate import tabulate\n","import pandas as pd\n","%matplotlib inline\n","print(tf.__version__)\n","print(tf.keras.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2.0.0\n","2.2.4-tf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"J_PFSyr9AP_l","outputId":"ddc00cc6-f640-48f1-b449-141ef6c70165","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1574699665435,"user_tz":-60,"elapsed":6853,"user":{"displayName":"Susi Handayani","photoUrl":"","userId":"17500954646245155216"}}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QH6OJuFvAA5o","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cwQIb1--cqJL","colab":{}},"source":["data = np.loadtxt('/content/drive/My Drive/thesis/CoST.csv',delimiter=',',skiprows=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q_eOrfpR5TJv","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"678f0798-dafd-430e-c47e-337cca702c14","executionInfo":{"status":"ok","timestamp":1574701125224,"user_tz":-60,"elapsed":1466610,"user":{"displayName":"Susi Handayani","photoUrl":"","userId":"17500954646245155216"}}},"source":["gestures_name = [\"grab\", \"hit\", \"massage\", \"pat\", \"pinch\", \"poke\", \"press\", \"rub\", \"scratch\", \"slap\", \"squeeze\", \"stroke\", \"tap\", \"tickle\"]\n","\n","subjects_train = [2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 16, 17, 20, 21, 22, 24, 25, 27, 28, 29, 30]\n","subjects_test = [5, 10, 18, 23, 31]\n","subjects_val = [15, 19, 26,  1,  4]\n","\n","max_pressure = 0\n","for row in data: \n","    for each in row[4:]:\n","        if each > max_pressure:\n","            max_pressure = each\n","print(max_pressure)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["990.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xOY-yW8dd0UH","colab":{}},"source":["def subsampling_generator(frame_length, subject):\n","    all_frames = []\n","    y = []\n","    subjects = []\n","\n","    for each in data: \n","        if each[0] == subject: \n","            subjects.append(each)\n","\n","    for n in range(len(gestures_name)):\n","        gestures = []\n","        for each in subjects:\n","            if each[2] == n + 1: \n","                gestures.append(each)\n","\n","        #rescale each channel by dividing with 990 (max number of pressure)\n","        #so the range of the pressure is between 0 and 1\n","        rescaling = []\n","        for each in gestures:\n","            rescaling.append(each[4:]/max_pressure)\n","            \n","\n","        #To find out index number of each gesture\n","        borders_index = []\n","        m = 0\n","        for each in gestures:\n","            if each[3] != 1:\n","                m += 1\n","            else:\n","                borders_index.append(int(m))\n","                m += 1\n","        borders_index = borders_index[1:]\n","        borders_index.append(len(gestures))\n","        \n","\n","        #To find out length of each gesture\n","        borders_length = []\n","        m = 0\n","        for each in gestures:\n","            if each[3] != 1:\n","                m += 1\n","            else:\n","                borders_length.append(int(m))\n","                m = 1\n","        borders_length.append(int(m)) #to save the last gestures\n","        borders_length = borders_length[1:] #remove 0 from the list because 0 is included in the list\n","\n","        #create subsampling\n","        j = 0\n","        k = 0\n","        frames = []\n","        for border_length in borders_length:\n","            if border_length > frame_length: #If one gesture has frames more than the set number are arranged as a subsample. \n","                iteration = int(border_length/frame_length)\n","                for i in range(iteration):\n","                    a = rescaling[j: j + frame_length]\n","                    for each in a:\n","                        frames.append(each)\n","                    for each in a:\n","                        all_frames.append(each)\n","                    j = j + frame_length\n","                b = rescaling[j: borders_index[k]]\n","                padded = frame_length - len(b)\n","                if padded != frame_length:\n","                    c = np.zeros((padded, 64))\n","                    b = np.vstack((b, c))\n","                for each in b:\n","                    frames.append(each)\n","                for each in b:\n","                        all_frames.append(each)\n","                j = borders_index[k]\n","            elif border_length < frame_length:  #If one gesture has fewer frames, then it is padded by zeros. \n","                b = rescaling[j: borders_index[k]]\n","                padded = frame_length - len(b)\n","                c = np.zeros((padded, 64))\n","                b = np.vstack((b, c))\n","                for each in b:\n","                    frames.append(each)\n","                for each in b:\n","                    all_frames.append(each)\n","                j = borders_index[k]\n","            else:   #If one gesture has same length as frame \n","                a = rescaling[j: borders_index[k]]\n","                for each in a:\n","                    frames.append(each)\n","                for each in a:\n","                    all_frames.append(each)\n","                j = borders_index[k]\n","            k = k + 1    \n","\n","\n","        #create label\n","        for i in range(len(frames)):\n","            y.append(n)\n","\n","    return all_frames, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MopN5j2Nu6AV","colab":{}},"source":["#reshape the dataset to the input shape that can be accepted by neural network\n","def create_dataset(subjects, frame_length):\n","    x_dataset = []\n","    y_dataset = []\n","    for subject in subjects:\n","        channels = []\n","        y = []\n","        channels, y = subsampling_generator(frame_length, subject)\n","        x_dataset.append(channels)\n","        y_dataset.append(y)\n","\n","    new_x_dataset = []\n","    for x in x_dataset:\n","        for each in x:\n","              new_x_dataset.append(each)\n","\n","    new_y_dataset = []\n","    for x in y_dataset:\n","        for each in x:\n","              new_y_dataset.append(each)\n","\n","    new_x_dataset = np.array(new_x_dataset)\n","    new_y_dataset = np.array(new_y_dataset)\n","\n","    xdataset = new_x_dataset.reshape(-1, frame_length, 8, 8, 1)\n","    new_y_dataset = new_y_dataset.reshape(-1, frame_length, 1)\n","\n","    ydataset = []\n","    for each in new_y_dataset:\n","          ydataset.append(each[1])\n","    ydataset = np.array(ydataset)\n","\n","    #create label\n","    ydataset = to_categorical(ydataset, num_classes = 14)\n","    ydataset = np.array(ydataset)\n","\n","    return xdataset, ydataset\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhaPJAmWaaao","colab_type":"text"},"source":["# 1 LAYER CONVLSTM"]},{"cell_type":"code","metadata":{"id":"wh2cK_A9aaar","colab_type":"code","colab":{}},"source":["frames = [50, 60, 70, 80, 90, 100]\n","filters = [64, 32, 16, 8]\n","results = []\n","results_dir = '/content/drive/My Drive/'\n","for frame_length in frames:\n","    for num_filter in filters:\n","        print(\"Number of filters:\", num_filter, \"Number of Frame Length: \", frame_length)\n","        xtrain, ytrain = create_dataset(subjects_train, frame_length)\n","        xval, yval = create_dataset(subjects_val, frame_length)\n","\n","        print(len(xtrain), len(ytrain), len(xval), len(yval))\n","        model = Sequential()\n","        model.add(ConvLSTM2D(filters=num_filter, kernel_size=(3, 3),\n","                     padding='same', recurrent_dropout=0.5, input_shape=(frame_length, 8, 8, 1)))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","        model.add(Flatten())\n","        model.add(Dropout(0.5, seed=1))\n","        model.add(Dense(14, activation='softmax'))\n","        optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","        model.compile(optimizer=optimizer,\n","                    loss='categorical_crossentropy',\n","                    metrics=['accuracy'])\n","\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","\n","        doc_name = str(frame_length) + \"_1convlstm_{epoch:03d}.h5\"\n","        print(\"Start frame_length = \", frame_length)\n","        checkpoint = ModelCheckpoint(os.path.join(results_dir, doc_name), monitor='loss', verbose=1, mode='auto', period=5)\n","        history = model.fit(xtrain, ytrain, epochs=100, verbose=0, batch_size=64, callbacks=[checkpoint], validation_data = (xval, yval))\n","        # convert the history.history dict to a pandas DataFrame:     \n","        hist_df = pd.DataFrame(history.history) \n","        #save to csv: \n","        doc_name = str(frame_length) + '_1convlstm_history.csv'\n","        with open(os.path.join(results_dir, doc_name), mode='w') as f:\n","            hist_df.to_csv(f)\n","print(\"Finish\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjrJ4O4jaaa2","colab_type":"text"},"source":["# 2 LAYER CONVLSTM"]},{"cell_type":"code","metadata":{"id":"KiqfwMiqaaa5","colab_type":"code","colab":{}},"source":["frames = [50, 60, 70, 80, 90, 100]\n","filters = [64, 32, 16, 8]\n","results = []\n","results_dir = '/content/drive/My Drive/'\n","for frame_length in frames:\n","    for num_filter in filters:\n","        print(\"Number of filters:\", num_filter, \"Number of Frame Length: \", frame_length)\n","        xtrain, ytrain = create_dataset(subjects_train, frame_length)\n","        xval, yval = create_dataset(subjects_val, frame_length)\n","\n","        print(len(xtrain), len(ytrain), len(xval), len(yval))\n","        model = Sequential()\n","        model.add(ConvLSTM2D(filters=num_filter, kernel_size=(3, 3),\n","                     padding='same', return_sequences = True, recurrent_dropout=0.5, input_shape=(frame_length, 8, 8, 1)))\n","        model.add(ConvLSTM2D(filters=num_filter, kernel_size=(3, 3),\n","                     padding='same', recurrent_dropout=0.5, input_shape=(frame_length, 8, 8, 1)))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","        model.add(Flatten())\n","        model.add(Dropout(0.5, seed=1))\n","        model.add(Dense(14, activation='softmax'))\n","        optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","        model.compile(optimizer=optimizer,\n","                    loss='categorical_crossentropy',\n","                    metrics=['accuracy'])\n","\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","\n","        doc_name = str(frame_length) + \"_2convlstm_{epoch:03d}.h5\"\n","        print(\"Start frame_length = \", frame_length)\n","        checkpoint = ModelCheckpoint(os.path.join(results_dir, doc_name), monitor='loss', verbose=1, mode='auto', period=5)\n","        history = model.fit(xtrain, ytrain, epochs=100, verbose=0, batch_size=64, callbacks=[checkpoint], validation_data = (xval, yval))\n","        # convert the history.history dict to a pandas DataFrame:     \n","        hist_df = pd.DataFrame(history.history) \n","        #save to csv: \n","        doc_name = str(frame_length) + '_2convlstm_history.csv'\n","        with open(os.path.join(results_dir, doc_name), mode='w') as f:\n","            hist_df.to_csv(f)\n","print(\"Finish\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xITs6OMSaabE","colab_type":"text"},"source":["# 1 LAYER OF BICONVLSTM"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CVbhljeVAuKA","colab":{}},"source":["frames = [50, 60, 70, 80, 90, 100]\n","filters = [64, 32, 16, 8]\n","results = []\n","results_dir = '/content/drive/My Drive/'\n","for frame_length in frames:\n","    for num_filter in filters:\n","        print(\"Number of filters:\", num_filter, \"Number of Frame Length: \", frame_length)\n","        xtrain, ytrain = create_dataset(subjects_train, frame_length)\n","        xval, yval = create_dataset(subjects_val, frame_length)\n","\n","        print(len(xtrain), len(ytrain), len(xval), len(yval))\n","        model = Sequential()\n","        model.add(Bidirectional(ConvLSTM2D(filters=num_filter, kernel_size=(3, 3),\n","                     padding='same', recurrent_dropout=0.5), input_shape=(frame_length, 8, 8, 1)))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","        model.add(Flatten())\n","        model.add(Dropout(0.5, seed=1))\n","        model.add(Dense(14, activation='softmax'))\n","        optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","        model.compile(optimizer=optimizer,\n","                    loss='categorical_crossentropy',\n","                    metrics=['accuracy'])\n","\n","\n","        doc_name = str(frame_length) + \"_1biconvlstm_{epoch:03d}.h5\"\n","        print(\"Start frame_length = \", frame_length)\n","        checkpoint = ModelCheckpoint(os.path.join(results_dir, doc_name), monitor='loss', verbose=1, mode='auto', period=5)\n","        history = model.fit(xtrain, ytrain, epochs=100, verbose=0, batch_size=64, callbacks=[checkpoint], validation_data = (xval, yval))\n","        # convert the history.history dict to a pandas DataFrame:     \n","        hist_df = pd.DataFrame(history.history) \n","        #save to csv: \n","        doc_name = str(frame_length) + '_1biconvlstm_history.csv'\n","        with open(os.path.join(results_dir, doc_name), mode='w') as f:\n","            hist_df.to_csv(f)\n","print(\"Finish\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LmLTVnv2ggxZ"},"source":["# 2 LAYERS OF BICONVLSTM"]},{"cell_type":"code","metadata":{"id":"HzwKCQuxaabZ","colab_type":"code","colab":{}},"source":["frames = [50, 60, 70, 80, 90, 100]\n","filters = [64, 32, 16, 8]\n","results = []\n","results_dir = '/content/drive/My Drive/'\n","for frame_length in frames:\n","    for num_filter in filters:\n","        print(\"Number of filters:\", num_filter, \"Number of Frame Length: \", frame_length)\n","        xtrain, ytrain = create_dataset(subjects_train, frame_length)\n","        xval, yval = create_dataset(subjects_val, frame_length)\n","\n","        print(len(xtrain), len(ytrain), len(xval), len(yval))\n","        model = Sequential()\n","        model.add(Bidirectional(ConvLSTM2D(filters=num_filter, kernel_size=(3, 3),\n","                     padding='same', return_sequences = True, recurrent_dropout=0.5), input_shape=(frame_length, 8, 8, 1)))\n","        model.add(Bidirectional(ConvLSTM2D(filters=num_filter, kernel_size=(3, 3),\n","                     padding='same', recurrent_dropout=0.5), input_shape=(frame_length, 8, 8, 1)))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","        model.add(Flatten())\n","        model.add(Dropout(0.5, seed=1))\n","        model.add(Dense(14, activation='softmax'))\n","        optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","        model.compile(optimizer=optimizer,\n","                    loss='categorical_crossentropy',\n","                    metrics=['accuracy'])\n","\n","\n","        doc_name = str(frame_length) + \"_2biconvlstm_{epoch:03d}.h5\"\n","        print(\"Start frame_length = \", frame_length)\n","        checkpoint = ModelCheckpoint(os.path.join(results_dir, doc_name), monitor='loss', verbose=1, mode='auto', period=5)\n","        history = model.fit(xtrain, ytrain, epochs=100, verbose=0, batch_size=64, callbacks=[checkpoint], validation_data = (xval, yval))\n","        # convert the history.history dictionary to a pandas DataFrame:     \n","        hist_df = pd.DataFrame(history.history) \n","        #save to csv: \n","        doc_name = str(frame_length) + '_2biconvlstm_history.csv'\n","        with open(os.path.join(results_dir, doc_name), mode='w') as f:\n","            hist_df.to_csv(f)\n","print(\"Finish\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UNnlRFNraabk","colab_type":"text"},"source":["# ACCURACY ON TEST SET"]},{"cell_type":"code","metadata":{"id":"6xarI6zzaabm","colab_type":"code","colab":{}},"source":["frame_length = 100\n","results = []\n","results_dir = \"C:/Jupyter/cost5/990/\"\n","print(\"frame length\", frame_length)\n","results = []\n","xtest, ytest = create_dataset(subjects_test, frame_length)\n","print(len(xtest), len(ytest))\n","doc_name = \"100_2biconvlstm32_85.h5\"\n","model = load_model(os.path.join(results_dir, doc_name))\n","score = model.evaluate(xtest, ytest, verbose=0)\n","results.append((frame_length, score[1], score[0]))\n","doc_name = str(frame_length) +'results.csv'\n","np.savetxt(os.path.join(results_dir, doc_name), results, delimiter=',', fmt='%s')\n","print(\"Finish\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6XtBfHKaabt","colab_type":"text"},"source":["# CREATE CONFUSION MATRIX"]},{"cell_type":"code","metadata":{"id":"M_ECEHFRaabz","colab_type":"code","colab":{}},"source":["def create_dataset_no_categorical(subjects, frame_length):\n","    x_dataset = []\n","    y_dataset = []\n","    for subject in subjects:\n","        channels = []\n","        y = []\n","        channels, y = subsampling_generator(frame_length, subject)\n","        x_dataset.append(channels)\n","        y_dataset.append(y)\n","\n","    new_x_dataset = []\n","    for x in x_dataset:\n","        for each in x:\n","              new_x_dataset.append(each)\n","\n","    new_y_dataset = []\n","    for x in y_dataset:\n","        for each in x:\n","              new_y_dataset.append(each)\n","\n","    new_x_dataset = np.array(new_x_dataset)\n","    new_y_dataset = np.array(new_y_dataset)\n","\n","    xdataset = new_x_dataset.reshape(-1, frame_length, 8, 8, 1)\n","    new_y_dataset = new_y_dataset.reshape(-1, frame_length, 1)\n","\n","    ydataset = []\n","    for each in new_y_dataset:\n","          ydataset.append(each[1])\n","    ydataset = np.array(ydataset)\n","\n","    return xdataset, ydataset\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k1A9VVgyaab7","colab_type":"text"},"source":["The script below is adapted from:\n","https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n"]},{"cell_type":"code","metadata":{"id":"cs38UBLAaab-","colab_type":"code","colab":{}},"source":["print(__doc__)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn import svm, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def plot_confusion_matrix(y_true, y_pred,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        cm = cm * 100\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    np.savetxt(\"confmat.csv\", cm, delimiter=',', fmt='%s')\n","    \n","    fig, ax = plt.subplots(figsize=(10, 10))\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()\n","    return ax"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFqFTiGraacG","colab_type":"code","colab":{}},"source":["frame_length = 100\n","results_dir = \"/content/drive/My Drive/\"\n","doc_name = \"100_2biconvlstm32_85.h5\"\n","xtest, ytest = create_dataset_no_categorical(subjects_test, frame_length)\n","model = load_model(os.path.join(results_dir, doc_name))\n","ypred = model.predict_classes(xtest)\n","ytest = np.array(ytest)\n","ypred = np.array(ypred)\n","print(len(ytest), len(ypred))\n","np.set_printoptions(precision=2)\n","\n","\n","# Plot normalized confusion matrix\n","plot_confusion_matrix(ytest, ypred, normalize=True,\n","                      title='Normalized confusion matrix')"],"execution_count":0,"outputs":[]}]}